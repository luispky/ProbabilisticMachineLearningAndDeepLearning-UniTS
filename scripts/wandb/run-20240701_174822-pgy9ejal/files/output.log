
Training...












Epoch: 62 | Loss: 0.0176:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 62/64 [00:25<00:00,  2.49it/s]
Training Finished
Sampling...
Sampling Finished
Min value: 0.0
Epoch: 64 | Loss: 0.0180: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [00:26<00:00,  2.43it/s]
128it [00:00, 900.34it/s]
Traceback (most recent call last):
  File "/home/l11/Documents/ProbabilisticMachineLearningProject/scripts/run_diffusion.py", line 274, in <module>
    main_sum_categorical_data()
  File "/home/l11/Documents/ProbabilisticMachineLearningProject/scripts/run_diffusion.py", line 208, in main_sum_categorical_data
    normalized_data = prob_instance.normalize(samples)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/l11/Documents/ProbabilisticMachineLearningProject/src/utils.py", line 559, in normalize
    assert np.all(s > 0), f'Zero sum: {s}'
           ^^^^^^^^^^^^^
AssertionError: Zero sum: [[1.         1.         1.05615807 ... 1.4331978  1.4331978  1.4331978 ]
 [0.96441785 0.96441785 0.86424473 ... 1.1100447  1.1100447  1.1100447 ]
 [1.07367383 1.07367383 1.29895931 ... 1.21258625 1.21258625 1.21258625]
 ...
 [1.         1.         1.20157228 ... 1.9063123  1.9063123  1.9063123 ]
 [1.         1.         0.89858446 ... 1.36301292 1.36301292 1.36301292]
 [0.9627361  0.9627361  0.77109224 ... 1.09807573 1.09807573 1.09807573]]